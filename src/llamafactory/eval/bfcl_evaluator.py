"""
BFCL Evaluator for LlamaFactory
ç®€å•åŒ…è£…åŸç”Ÿ BFCL å‘½ä»¤
"""

import os
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, Optional


class BFCLEvaluator:
    """
    LlamaFactory çš„ BFCL è¯„ä¼°å™¨
    ç›´æ¥è°ƒç”¨åŸç”Ÿ BFCL å‘½ä»¤
    """
    
    def __init__(self, args: Optional[Dict[str, Any]] = None):
        self.args = args or {}
        
        # BFCL æ ¹ç›®å½•
        self.bfcl_root = Path(__file__).parent / "bfcl" / "berkeley-function-call-leaderboard"
        
        # æå–å‚æ•°
        self.model_name = self.args.get("model_name_or_path", "")
        self.test_category = self.args.get("bfcl_category", "multiple")
        self.port = self.args.get("bfcl_port", 8000)
        self.api_base = self.args.get("bfcl_api_base", "http://localhost")
        self.num_threads = self.args.get("bfcl_num_threads", 1)
        self.include_input_log = self.args.get("bfcl_include_input_log", False)
        self.run_ids = self.args.get("bfcl_run_ids", False)
        self.partial_eval = self.args.get("bfcl_partial_eval", False)
        self.stage = self.args.get("bfcl_stage", "all")
        
        custom_result_dir = self.args.get("bfcl_result_dir", "")  
        custom_score_dir = self.args.get("bfcl_score_dir", "")
        
        if custom_result_dir:
            self.result_dir = Path(custom_result_dir)
        else:
            # ä½¿ç”¨ BFCL é»˜è®¤è·¯å¾„ï¼šä¸æŒ‡å®šï¼Œè®© BFCL è‡ªå·±å¤„ç†
            self.result_dir = None
        
        if custom_score_dir:
            self.score_dir = Path(custom_score_dir)
        else:
            # ä½¿ç”¨ BFCL é»˜è®¤è·¯å¾„ï¼šä¸æŒ‡å®šï¼Œè®© BFCL è‡ªå·±å¤„ç†
            self.score_dir = None
        
        # ç»“æœç›®å½•
        # model_safe_name = os.path.basename(self.model_name).replace("/", "_")
        # self.result_dir = Path.cwd() / "bfcl_results" / model_safe_name / "result"
        # self.score_dir = Path.cwd() / "bfcl_results" / model_safe_name / "score"
        
        print(f"\n{'='*70}")
        print(f"ğŸ”§ BFCL Evaluator Initialized")
        print(f"{'='*70}")
        print(f"Model: {self.model_name}")
        print(f"Stage: {self.stage}")
        print(f"Test Category: {self.test_category}")
        print(f"API: {self.api_base}:{self.port}")
        print(f"Result Dir: {self.result_dir}")
        print(f"Score Dir: {self.score_dir}")
        print(f"{'='*70}\n")
    
    def _setup_environment(self) -> Dict[str, str]:
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        env = os.environ.copy()
        base_url = f"{self.api_base}:{self.port}/v1"
        
        # BFCL éœ€è¦çš„ç¯å¢ƒå˜é‡
        env["OPENAI_API_BASE"] = base_url
        env["OPENAI_API_KEY"] = "EMPTY"
        
        env["LLAMAFACTORY_BASE_URL"] = base_url
        env["LLAMAFACTORY_API_KEY"] = "EMPTY"
        env["LLAMAFACTORY_PORT"] = str(self.port)
        
        # é¡¹ç›®æ ¹ç›®å½•
        env["BFCL_PROJECT_ROOT"] = str(self.bfcl_root)
        
        return env
    
    def _run_bfcl_command(self, command: list, description: str) -> bool:
        """
        è¿è¡ŒåŸç”Ÿ BFCL å‘½ä»¤
        
        Args:
            command: BFCL å‘½ä»¤å‚æ•°åˆ—è¡¨
            description: å‘½ä»¤æè¿°
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ {description}")
        print(f"{'='*70}\n")
        
        env = self._setup_environment()
        
        # âœ… ç›´æ¥è°ƒç”¨ bfcl å‘½ä»¤
        full_command = ["bfcl"] + command
        
        print(f"Command: {' '.join(full_command)}\n")
        
        try:
            result = subprocess.run(
                full_command,
                env=env,
                cwd=str(self.bfcl_root),
                check=False  # ä¸è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸
            )
            
            if result.returncode == 0:
                print(f"\nâœ… {description} completed successfully")
                return True
            else:
                print(f"\nâŒ {description} failed with return code {result.returncode}")
                return False
        
        except FileNotFoundError:
            print(f"\nâŒ 'bfcl' command not found")
            print("Please ensure BFCL is installed:")
            print(f"  cd {self.bfcl_root}")
            print("  pip install -e .")
            return False
        
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate(self) -> bool:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        if self.result_dir is not None:
            self.result_dir.mkdir(parents=True, exist_ok=True)
            
        # æ„å»ºå‘½ä»¤
        command = [
            "generate",
            "--model", self.model_name,  
            "--test-category", self.test_category,
            "--num-threads", str(self.num_threads),
        ]
        
        if self.result_dir:
            command.append("--result-dir")
            command.append(str(self.result_dir))
        
        if self.include_input_log:
            command.append("--include-input-log")
        
        if self.run_ids:
            command.append("--run-ids")
        
        return self._run_bfcl_command(command, "Generate")
    
    def evaluate(self) -> bool:
        """è¯„ä¼°ç”Ÿæˆçš„å“åº”"""
        # ç¡®ä¿è¯„åˆ†ç›®å½•å­˜åœ¨
        if self.score_dir is not None:
            self.score_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºå‘½ä»¤
        command = [
            "evaluate",
            "--model", self.model_name,
            "--test-category", self.test_category,
        ]
        
        if self.result_dir:
            command.append("--result-dir")
            command.append(str(self.result_dir))
        
        if self.score_dir:
            command.append("--score-dir")
            command.append(str(self.score_dir))
        
        if self.partial_eval:
            command.append("--partial-eval")
        
        return self._run_bfcl_command(command, "Evaluate")
    
    def show_scores(self) -> bool:
        """æ˜¾ç¤ºè¯„åˆ†"""
        command = [
            "scores",
        ]
        
        return self._run_bfcl_command(command, "Scores")
    
    def run_stage(self) -> Dict[str, Any]:
        """æ ¹æ® stage å‚æ•°æ‰§è¡Œå¯¹åº”é˜¶æ®µ"""
        
        if self.stage == "generate":
            print(f"\n{'='*70}")
            print(f"ğŸ¯ Running Stage: Generate Only")
            print(f"{'='*70}\n")
            
            if not self.generate():
                return {"status": "failed", "stage": "generate"}
            
            return {"status": "success", "stage": "generate"}
        
        elif self.stage == "evaluate":
            print(f"\n{'='*70}")
            print(f"ğŸ¯ Running Stage: Evaluate Only")
            print(f"{'='*70}\n")
            
            if not self.evaluate():
                return {"status": "failed", "stage": "evaluate"}
            
            return {"status": "success", "stage": "evaluate"}
        
        elif self.stage == "scores":
            print(f"\n{'='*70}")
            print(f"ğŸ¯ Running Stage: Show Scores Only")
            print(f"{'='*70}\n")
            
            if not self.show_scores():
                return {"status": "failed", "stage": "scores"}
            
            return {"status": "success", "stage": "scores"}
        
        elif self.stage == "all":
            return self.run_full_evaluation()
        
        else:
            print(f"âŒ Unknown stage: {self.stage}")
            return {"status": "failed", "error": f"Unknown stage: {self.stage}"}
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Running Full BFCL Evaluation (All Stages)")
        print(f"{'='*70}\n")
        
        # 1. Generate
        if not self.generate():
            return {"status": "failed", "stage": "generate", "error": "Generate failed"}
        
        # 2. Evaluate
        if not self.evaluate():
            return {"status": "failed", "stage": "evaluate", "error": "Evaluate failed"}
        
        # 3. Show scores
        # self.show_scores()
        
        # è®¡ç®—å®é™…è·¯å¾„
        model_safe_name = self.model_name.replace("/", "_").replace("\\", "_")
        
        if self.result_dir is not None:
            actual_result_dir = self.result_dir
        else:
            actual_result_dir = self.bfcl_root / "result" / model_safe_name
        
        if self.score_dir is not None:
            actual_score_dir = self.score_dir
        else:
            actual_score_dir = self.bfcl_root / "score" / model_safe_name
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'='*70}")
        print(f"âœ… Full Evaluation Completed")
        print(f"{'='*70}")
        print(f"ğŸ“ Results: {actual_result_dir}")
        print(f"ğŸ“Š Scores: {actual_score_dir}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        if actual_result_dir.exists():
            result_files = list(actual_result_dir.glob("*.json"))
            print(f"   Generated {len(result_files)} result file(s)")
        
        if actual_score_dir.exists():
            score_files = list(actual_score_dir.glob("*.json"))
            print(f"   Generated {len(score_files)} score file(s)")
        
        print(f"{'='*70}\n")
        
        return {
            "status": "success",
            "result_dir": str(actual_result_dir),
            "score_dir": str(actual_score_dir),
        }



def run_bfcl_eval(args: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    è¿è¡Œ BFCL è¯„ä¼°çš„å…¥å£å‡½æ•°
    """
    evaluator = BFCLEvaluator(args)
    return evaluator.run_stage()