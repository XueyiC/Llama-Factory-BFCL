# ============================================================
# LlamaFactory + BFCL + vLLM 完整一步安装
# Author: XueyiC
# Repository: https://github.com/XueyiC/Llama-Factory-BFCL
# Python: >=3.10
# ============================================================

# ============================================================
# 本地包安装（开发模式）
# ============================================================

# 安装 BFCL 包（包含 bfcl 命令行工具）
-e ./src/llamafactory/eval/bfcl/berkeley-function-call-leaderboard

# 安装 LlamaFactory 包（包含 llamafactory-cli 命令行工具）
-e .

# ============================================================
# Core LLM Framework
# ============================================================
transformers>=4.49.0,<=4.56.2
datasets>=2.16.0,<=4.0.0
accelerate>=1.3.0,<=1.11.0
peft>=0.14.0,<=0.17.1
trl>=0.8.6,<=0.9.6

# ============================================================
# GUI Components
# ============================================================
gradio>=4.38.0,<=5.45.0
matplotlib>=3.7.0

# ============================================================
# Core Operations
# ============================================================
einops
numpy==1.26.4
pandas>=2.0.0
scipy

# ============================================================
# Model and Tokenizer
# ============================================================
sentencepiece
tiktoken
modelscope>=1.14.0
hf-transfer
safetensors<=0.5.3

# ============================================================
# Python Utilities
# ============================================================
fire
omegaconf
packaging
protobuf
pyyaml
pydantic>=2.8.2,<=2.10.6

# ============================================================
# API Server
# ============================================================
uvicorn
fastapi
sse-starlette

# ============================================================
# Media Processing
# ============================================================
av
librosa

# ============================================================
# BFCL Core Dependencies
# ============================================================
requests
tqdm
pathlib
huggingface_hub
python-dotenv>=1.0.1

# ============================================================
# Tree-sitter (Code Parsing)
# ============================================================
tree_sitter==0.21.3
tree-sitter-java==0.21.0
tree-sitter-javascript==0.21.4

# ============================================================
# LLM API Clients (BFCL)
# ============================================================
openai>=1.86.0
mistralai==1.7.0
anthropic>=0.61.0
cohere==5.18.0
google-genai==1.24.0
qwen-agent

# ============================================================
# CLI and Utilities (BFCL)
# ============================================================
typer>=0.12.5
tabulate>=0.9.0
datamodel-code-generator==0.25.7
mpmath==1.3.0
tenacity>=8.5.0
writer-sdk>=2.1.0
overrides

# ============================================================
# AWS and Web Scraping (BFCL)
# ============================================================
boto3
beautifulsoup4
html2text

# ============================================================
# Search and Retrieval (BFCL)
# ============================================================
rank_bm25==0.2.2
google-search-results
sentence-transformers>=2.7.0
faiss-cpu==1.11.0
networkx==3.3

# ============================================================
# vLLM High-Performance Inference
# ============================================================
vllm==0.8.5

# ============================================================
# Optional Dependencies (uncomment if needed)
# ============================================================

# PyTorch (usually installed with vllm, but can specify version)
# torch>=2.1.0,<2.5.0
# torchvision>=0.16.0

# Quantization
# bitsandbytes>=0.41.0,<=0.44.0
# auto-gptq>=0.7.0
# autoawq>=0.2.0

# Distributed Training
# deepspeed>=0.10.0

# Experiment Tracking
# wandb==0.18.5
# tensorboard>=2.14.0

# Additional Inference Engines
# sglang  # For SGLang support (uncomment from BFCL optional deps)

# Development Tools
# pytest>=7.4.0
# pytest-cov>=4.1.0
# black>=23.0.0
# flake8>=6.0.0
# mypy>=1.0.0

# Documentation
# sphinx>=7.0.0
# sphinx-rtd-theme>=1.3.0

# ============================================================
# Installation Notes
# ============================================================
#
# Install everything with one command:
#   pip install -r requirements_all.txt
#
# This will:
#   1. Install BFCL package with 'bfcl' command
#   2. Install LlamaFactory package with 'llamafactory-cli' command
#   3. Install all dependencies including vLLM
#
# Verify installation:
#   llamafactory-cli --help
#   bfcl --help
#   python -c "import vllm; print(vllm.__version__)"
#
# Supported Python versions: >=3.10
# Recommended: Python 3.10 or 3.11
#
# GPU Support:
#   - CUDA >= 11.8 recommended for vLLM
#   - CUDA >= 12.1 for best performance
#
# For CPU-only installation (without vLLM):
#   Comment out the vLLM line above
#
# ============================================================